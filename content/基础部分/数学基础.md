> 前言：此部分文章并非系统性的教学文章，网络上已经很多非常优秀的教学课程，顶尖且免费。比如，哔哩哔哩上李沐大神的《动手学深度学习v2》，Andrej Karpathy 在 YouTube 上教程，以及 Standford CS224N 课程。
> 这里主要记录作者的一些理解，有意思的知识点，或者豁然开朗的乐趣，希望你也能喜欢！
### 数学基础

#### 线性代数

有些材料把行列式默认为线性代数是有偏差的，虽然大学本科阶段学习的线性代数，大部分都与行列式的计算有关。

行列式又称矩阵，可以看做是二阶张量；向量是一阶张量；标量则是零阶张量。当然还有更高纬度的张量，大语言模型会用到，Pytorch 也支持。

大语言模型的训练和推理所需绝大部分的计算，就是张量计算。Nvidia GPU 通过 CUDA 架构加速的也就是这部分计算。深度学习算法使用的数学表达式，其中的很多变量本身就是一个张量。

因此张量四则运算、内积等是入门必备。

##### 张量的物理含义

很多情况下，使用张量是因为方便，易于理解。比如批量样本，文本字符串，在我们平时编程过程中会自然的用到数组来表示，这时张量也就如果数组一般知识个容器；其实，大部分机器学习框架中，张量和数组是可以互相转换的，多维张量在数据表达方面一定程度上等同于多维数组，只是张量提供了更多操作语义和更好的计算性能。

包括上述例子中提到的批量样本，文本字符串等大多数场景下，张量作为容器而言没有其他的物理表征意义。

而深度学习中涉及的 embedding 张量，可以理解为自然语言在经过 tokenization 后，将 token 投射到多维空间；后续可以产生很多应用，比如 RAG 技术中，提问（或者搜索）与内容相似度的匹配，如果是二维向量，求解的就是两者之间的余弦夹角。

高阶张量点乘的数学含义：[度量张量的本质及详解（高阶）——全网最好的讲解](https://zhuanlan.zhihu.com/p/190104245)

#### 概率统计

线性代数主要用于高性能的数值计算，特别是大量使用 GPU 的大语言模型，集中在矩阵计算的计算加速上。Transformer 模型中 90% 以上的计算量在于矩阵计算，并且此模型提供了一种并发计算的能力，这也是它除了 Attention 机制外，优于其他深度学习模型的原因。

概率统计在机器学习的各个层面都会用到，比如前期的数据探索，相关性分析；中期的模型算法，比如损失函数的选择；后期的模型评估，必须通过统计检验，具备统计学意义。

##### 极大似然估计

首先需要了解什么是条件概率。条件概率 $P(y/X)$ 是 X 发生情况下，y 发生的概率。获取到一个数据样本，样本中的每条数据都包含了自变量 X 集合值，以及对应的应变量 y 的值。

极大似然估计，就是基于特定模型假设（比如线性回归模型），计算满足现有样本数据出现的最大联合概率下，该模型的具体参数值。

[为什么是“最小二乘”而不是“最小一乘”？](https://zhuanlan.zhihu.com/p/104391088)

线性回归模型中，基于正态分布的最大似然估计，就是最小二乘法。

##### 联合概率

在不同假设下，联合概率的计算方式有所不同。

- 独立样本的联合概率（为了简化模型和计算，对大部分数据样本做了独立假设）

$$ P(X） = P(x_1) * P(x_2) * ... * P(x_n) $$

- 非独立样本的联合概率（序列数据，比如语言文本）

$$ P(X) = P(x_1) * P(x_2/x_1) * P(x_3/x_1x_2)...*P(x_n/x_1...X_n)$$

#### 微积分

梯度下降算法，是微积分现下最经典的应用。Pytorch 和 Tensorflow，都实现了自动微分求导。 Pytorch Autograd 采用了动态计算图，而 Tensorflow Automatic Differentiation 使用的是静态计算图。

##### 函数求导的链式法则

$$ \frac{\mathrm{d}f}{\mathrm{d}x} = \frac{\mathrm{d}f}{\mathrm{d}g} * \frac{\mathrm{d}g}{\mathrm{d}x} $$

计算图的梯度后向传播算法就是依据链式法则实现的。

上述三门理工科基础数学课，应用于深度学习中的方方面面。神经网络算法中到处可见张量，及其相关计算的身影。概率统计，在人工智能的各个阶段都发挥着不可替代的作用。微积分，为各种算法提供解决计算方案。
